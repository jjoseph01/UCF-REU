{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ9Ne1D2yCF5XimjU8R0Wa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjoseph01/UCF-REU/blob/main/REU_initial_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need packages\n",
        "\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL5m1AfZqCB7",
        "outputId": "ce3c2214-2b40-4fc5-bc84-2b3385c177e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m499.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-33hbkrpi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-33hbkrpi\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.18.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=7b57b1b4d956cc0aa8585da590997c2886690ca154df0bc3bff845fbd2c78c3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-56r7t069/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m0NxDAkgp2YY"
      },
      "outputs": [],
      "source": [
        "# the model\n",
        "\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AvgPool(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.mean(dim=1)  # (N,F,512) -> (N,512)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerAggregator(nn.Module):\n",
        "    def __init__(self, d_model=512, nhead=4, num_layers=1, dim_feedforward=512, dropout=0.0):\n",
        "        super(TransformerAggregator, self).__init__()\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, num_frames, d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.transpose(0, 1)  # Transformer expects (S, N, E) shape\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.transpose(0, 1)  # Back to (batch_size, num_frames, d_model)\n",
        "        x = self.mlp(x)\n",
        "        x = torch.mean(x, dim=1)  # Average embeddings to get a single embedding\n",
        "        return x\n",
        "\n",
        "\n",
        "class TimePredVid(torch.nn.Module):\n",
        "    def __init__(self, aggregator='mean', precomp=False, device='cpu'):\n",
        "        '''\n",
        "        aggregate_frames: 'mean', 'transformer'\n",
        "        input: 'frames', 'features'\n",
        "        device: 'cpu', 'cuda'\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.aggregator = aggregator\n",
        "        self.precomp = precomp\n",
        "        self.device = device\n",
        "\n",
        "        self.backbone, _ = clip.load(\"ViT-L/14\", device=device)\n",
        "\n",
        "        if aggregator == 'mean':\n",
        "            self.aggregator = AvgPool()\n",
        "        elif aggregator == 'transformer':\n",
        "            self.aggregator = TransformerAggregator()\n",
        "\n",
        "        self.cls_head = nn.Linear(512, 24)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.precomp:\n",
        "            x = self.backbone(x)\n",
        "        x = self.aggregator(x)\n",
        "        x = self.cls_head(x)\n",
        "        return x\n",
        "\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = TimePredVid(aggregator='mean', precomp=False, device=device).to(device) #same as clip practice\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  #not needed for newton\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLrjXJFq6hI6",
        "outputId": "9424388b-5769-4108-f627-8464a3edd0f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/bdd_sample/bdd_sample' #not needed for newton"
      ],
      "metadata": {
        "id": "WrqV-zJp6nad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "class BDDDataset(Dataset):\n",
        "    def __init__(self, data, dataset_dir, sample_n_frames=10, transform=None):\n",
        "        self.data = data\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.sample_n_frames = sample_n_frames\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        file_name = row['file_name']\n",
        "        date_str = row['date']\n",
        "        train_or_val = 'train' if row['train_or_val'] == 1 else 'val'\n",
        "        file_path = os.path.join(self.dataset_dir, f'Ground/{train_or_val}/{file_name}')\n",
        "\n",
        "        # Load all frames\n",
        "        frame_files = sorted([f for f in os.listdir(file_path) if f.endswith('.jpg')], key=lambda x: int(x.split('_')[0]))\n",
        "        frames = [Image.open(os.path.join(file_path, frame_file)) for frame_file in frame_files]\n",
        "\n",
        "        # Sample n evenly spaced frames\n",
        "        indices = np.linspace(0, len(frames) - 1, self.sample_n_frames, dtype=int)\n",
        "        sampled_frames = [frames[i] for i in indices]\n",
        "\n",
        "        if self.transform:\n",
        "            sampled_frames = [self.transform(frame) for frame in sampled_frames]\n",
        "\n",
        "        frames_tensor = torch.stack(sampled_frames)\n",
        "\n",
        "        # Extract month and hour\n",
        "        date_parts = pd.to_datetime(date_str)\n",
        "        month = date_parts.month\n",
        "        day = date_parts.day\n",
        "        hour = date_parts.hour\n",
        "        minute = date_parts.minute\n",
        "        second = date_parts.second\n",
        "\n",
        "        # Calculate continuous time representations\n",
        "        m = (month - 1) + ((day - 1) * 12 / 365)\n",
        "        h = hour + (minute / 60) + (second / 3600)\n",
        "\n",
        "        time_tensor = torch.tensor([m, h], dtype=torch.float)\n",
        "\n",
        "        return frames_tensor, time_tensor\n",
        "\n",
        "\n",
        "def create_bdd_datasets(dataset_dir, sample_n_frames=10, train_transform=None, val_transform=None, dataset_sample=1.0, seed=23):\n",
        "    csv_file = os.path.join(dataset_dir, 'metadata.csv')\n",
        "\n",
        "    # Load CSV\n",
        "    data = pd.read_csv(csv_file)\n",
        "\n",
        "    # Sample dataset for debugging if dataset_sample < 1.0\n",
        "    if dataset_sample < 1.0:\n",
        "        data = data.sample(frac=dataset_sample, random_state=seed)\n",
        "\n",
        "    # Split into train and val\n",
        "    train_data = data[data['train_or_val'] == 1]\n",
        "    val_data = data[data['train_or_val'] == 0]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BDDDataset(train_data, dataset_dir, sample_n_frames, train_transform)\n",
        "    val_dataset = BDDDataset(val_data, dataset_dir, sample_n_frames, val_transform)\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "lvOcUxKh5GKu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#insert dataloader code or start training + evaluating?\n",
        "def train_model(model, dataloader, criterion, optimizer, device, num_epochs=1):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for frames, labels in tqdm(dataloader):\n",
        "            frames = frames.view(-1, frames.size(2), frames.size(3), frames.size(4))  # Reshape to [batch_size * num_frames, channels, height, width]\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "            image_embeddings = model.backbone.encode_image(frames)\n",
        "\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(image_embeddings)\n",
        "            loss = criterion(outputs, times)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in tqdm(dataloader):\n",
        "            frames = frames.view(-1, frames.size(2), frames.size(3), frames.size(4))  # Reshape to [batch_size * num_frames, channels, height, width]\n",
        "            frames, labels = frames.to(device), times.to(device)\n",
        "\n",
        "            image_embeddings = model.backbone.encode_image(frames)\n",
        "\n",
        "            outputs = model(image_embeddings)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == times).sum().item()\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
        "# evaluate_model(model, val_loader, device)"
      ],
      "metadata": {
        "id": "kcMu5_y--oLK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # Usage\n",
        "    dataset_dir = '/content/drive/MyDrive/bdd_sample/bdd_sample' #'/home/c3-0/sarucrcv/geo3/BDD100k_Big'\n",
        "    batch_size = 4\n",
        "    num_workers = 1 #orginally 8\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_dataset, val_dataset = create_bdd_datasets(\n",
        "        dataset_dir,\n",
        "        sample_n_frames=10,\n",
        "        train_transform=transform,\n",
        "        val_transform=transform,\n",
        "        dataset_sample=0.5,\n",
        "        seed=23\n",
        "    )\n",
        "\n",
        "    print(len(train_dataset))\n",
        "    print(len(val_dataset))\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    for frames, times in train_loader:\n",
        "        print(frames.shape)\n",
        "        print(times.shape)\n",
        "        print(times)\n",
        "        break\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = TimePredVid(aggregator='mean', precomp=False, device=device).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_model(model, train_loader, criterion, optimizer, device, num_epochs=1)\n",
        "    evaluate_model(model, val_loader, device)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o31ie7LGRHf",
        "outputId": "9e93dd96-6717-4269-b510-57f7d019c375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "2\n",
            "torch.Size([4, 10, 3, 224, 224])\n",
            "torch.Size([4, 2])\n",
            "tensor([[ 8.0000, 17.0036],\n",
            "        [ 7.9534, 13.6186],\n",
            "        [ 9.0986, 17.9950],\n",
            "        [ 7.9863,  0.3186]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}